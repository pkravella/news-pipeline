#!/usr/bin/env python3
import os, sys, re, json, time
from typing import List, Dict, Tuple
import yaml
from confluent_kafka import Consumer, Producer
import spacy
from rapidfuzz import fuzz, process

# Protobuf (generated by scripts/gen_protos_py.sh)
sys.path.append(os.path.join(os.path.dirname(__file__), "pb"))
import news_pb2

CASHTAG_RE = re.compile(r'\$([A-Z]{1,5})\b')

def load_config(path: str) -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)

def load_tickers(csv_path: str) -> Tuple[Dict[str, dict], List[str]]:
    """
    Returns:
      tickers_by_symbol: { "AAPL": {"name": "Apple Inc", "aliases": ["Apple","Apple Computer",...] } }
      alias_index: list of (alias -> symbol) via fuzzy matching on demand
    """
    tickers = {}
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"tickers csv not found: {csv_path}")

    import csv
    with open(csv_path, newline='', encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            sym = row["symbol"].strip().upper()
            name = row.get("name", "").strip()
            aliases = [a.strip() for a in (row.get("aliases","") or "").split(";") if a.strip()]
            aliases = list(dict.fromkeys([name] + aliases)) if name else aliases
            tickers[sym] = {"name": name, "aliases": aliases}
    # Build a list of all alias strings for fuzzy lookup
    all_aliases = []
    for sym, meta in tickers.items():
        for alias in meta["aliases"]:
            all_aliases.append(alias)
    return tickers, all_aliases

def extract_cashtags(text: str) -> List[str]:
    return list(set([m.group(1) for m in CASHTAG_RE.finditer(text or "")]))

def host_from_url(url: str) -> str:
    if "://" in url:
        host = url.split("://",1)[1].split("/",1)[0]
    else:
        host = url.split("/",1)[0]
    return host.lower()

def score_ticker(symbol: str, title: str, body: str, hints: List[str], detected_cashtags: List[str], aliases_for_sym: List[str]) -> float:
    s = 0.0
    # cashtag
    if symbol in detected_cashtags:
        s += 2.0
    # title mention (symbol or alias)
    t_low = (title or "").lower()
    found_title = False
    if symbol.lower() in t_low:
        found_title = True
    else:
        for alias in aliases_for_sym:
            if alias and alias.lower() in t_low:
                found_title = True; break
    if found_title: s += 1.5

    # domain context (company PR or recognizable host)
    domain_boost = 0.0
    for h in hints or []:
        if h.startswith("host:"):
            host = h.split(":",1)[1]
            # simple source-based priors
            if "ir." in host or "investor" in host or host.endswith(".com") and symbol.lower() in host:
                domain_boost = 1.0
    s += domain_boost

    # NER match added separately by resolver()
    return s

def topic_rules(title: str, body: str) -> List[str]:
    t = (title or "").lower()
    b = (body or "").lower()
    topics = set()
    if any(w in t or w in b for w in ["guidance","outlook","forecast"]):
        topics.add("guidance")
    if any(w in t or w in b for w in ["earnings","q1","q2","q3","q4","fiscal","full-year","eps","revenue","beat","miss"]):
        topics.add("earnings")
    if any(w in t or w in b for w in ["acquire","acquisition","merger","m&a","takeover"]):
        topics.add("m&a")
    if any(w in t or w in b for w in ["lawsuit","litigation","settlement","sues","sued","investigation"]):
        topics.add("legal")
    if any(w in t or w in b for w in ["downgrade","upgrade","initiates coverage","price target"]):
        topics.add("rating/analyst")
    if any(w in t or w in b for w in ["macro","inflation","cpi","jobs report","fed","interest rates","treasury"]):
        topics.add("macro")
    if any(w in t or w in b for w in ["product","launches","introduces","unveils","chip","ai model","device","phone","gpu"]):
        topics.add("product")
    return sorted(topics)

def resolver():
    cfg = load_config(os.path.join("config","entity.yml"))
    kcfg = cfg["kafka"]
    rcfg = cfg["resolver"]

    # Kafka
    consumer = Consumer({
        "bootstrap.servers": kcfg["bootstrap_servers"],
        "group.id": kcfg["group_id"],
        "auto.offset.reset": "latest",
        "enable.auto.commit": True
    })
    consumer.subscribe([kcfg["topic_in"]])

    producer = Producer({"bootstrap.servers": kcfg["bootstrap_servers"]})

    # spaCy
    nlp = spacy.load("en_core_web_sm")

    # Tickers
    tickers, alias_list = load_tickers(rcfg["tickers_csv"])
    symbol_set = set(tickers.keys())

    print(f"[entity] Ready. Consuming '{kcfg['topic_in']}' -> producing '{kcfg['topic_out']}'")

    while True:
        msg = consumer.poll(0.2)
        if msg is None:
            continue
        if msg.error():
            print(f"[entity] Kafka error: {msg.error()}", flush=True)
            continue

        try:
            clean = news_pb2.ArticleClean()
            clean.ParseFromString(msg.value())
        except Exception as e:
            print(f"[entity] Parse error: {e}", flush=True)
            continue

        title = clean.title or ""
        body = clean.body or ""
        hints = list(clean.hints)

        # Cashtags
        cashtags = extract_cashtags(title + " " + body)

        # NER (ORG/PRODUCT)
        doc = nlp((title + "\n" + body[:3000]).strip())
        orgs = [e.text for e in doc.ents if e.label_ in ("ORG","PRODUCT")]

        # Fuzzy map org -> ticker candidates
        # We compute a score per symbol using cashtag/title/domain/NER.
        scores = {}
        # Pre-seed with cashtags
        for sym in cashtags:
            if sym in symbol_set:
                scores[sym] = scores.get(sym, 0.0) + 2.0

        # For each org mention, fuzzy search against alias list
        for org in orgs:
            match, score, idx = process.extractOne(org, alias_list, scorer=fuzz.token_set_ratio)
            if score >= 85:
                # find which symbol owns that alias
                for sym, meta in tickers.items():
                    if match in meta["aliases"]:
                        scores[sym] = scores.get(sym, 0.0) + 1.0  # NER contribution
                        break

        # Title/domain/etc score
        for sym, meta in tickers.items():
            s = score_ticker(sym, title, body, hints, cashtags, meta["aliases"])
            if s > 0:
                scores[sym] = scores.get(sym, 0.0) + s

        # Keep top tickers above threshold
        pairs = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
        kept = [sym for sym, sc in pairs if sc >= float(rcfg["min_score"])][:int(rcfg["max_tickers"])]

        # Topics (basic rules for now)
        topics = topic_rules(title, body)

        enriched = news_pb2.ArticleEnriched()
        enriched.id = clean.id
        enriched.title = clean.title
        enriched.body = clean.body
        enriched.published_ts = clean.published_ts
        enriched.tickers.extend(kept)
        enriched.entities.extend(orgs)
        enriched.topics.extend(topics)

        out_bytes = enriched.SerializeToString()
        producer.produce(kcfg["topic_out"], key=clean.id.encode("utf-8"), value=out_bytes)
        producer.poll(0)

